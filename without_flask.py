# -*- coding: utf-8 -*-
"""without flask.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U0X_o4FsLrto1Mq6kUPH__9OJxMlQPt2

## <div style="border-radius:0px; border:#3eb489 solid; padding: 15px; background-color: #ddefdd; font-size:100%; text-align:center">1. Import Libraries</div>
"""


# Commented out IPython magic to ensure Python compatibility.
# data
import pandas as pd
import numpy as np


# visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno # Ù…ÙƒØªØ¨Ø© Ù…ØªØ®ØµØµØ© ÙÙŠ ØªØµÙˆÙŠØ± Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©

import plotly.express as px #Ù„Ø¹Ù…Ù„ Ø±Ø³ÙˆÙ…Ø§Øª Interactive Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø³Ø·Ø± ÙˆØ§Ø­Ø¯.

import plotly.figure_factory as ff # Figure Factory Ù„Ø±Ø³ÙˆÙ…Ø§Øª Ù…ØªØ®ØµØµØ© Ù…Ø´ Ù…ØªØ§Ø­Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ px >> Ø²ÙŠ ØªÙˆØ²ÙŠØ¹Ø§Øª Ùˆ Ø§Ù„ÙƒØ«Ø§ÙÙ‡ Ù…Ø«Ù„Ø§
import plotly.graph_objects as go

from wordcloud import WordCloud # Ù†Ø³ØªØ®Ø¯Ù…Ù‡Ø§ Ù„Ø¥Ø¹Ø·Ø§Ø¡ Ù„Ù…Ø­Ø© Ø³Ø±ÙŠØ¹Ø© Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ù‹Ø§ ÙÙŠ Ù†ØµÙˆØµ

# styling
# %matplotlib inline
sns.set_style('darkgrid')
mpl.rcParams['font.size'] = 14
mpl.rcParams['figure.facecolor'] = '#00000000'
mpl.rcParams['font.size'] = 14
mpl.rcParams['figure.facecolor'] = '#00000000'

import os
from wordcloud import WordCloud

import warnings
warnings.filterwarnings("ignore")

"""## <div style="border-radius:0px; border:#3eb489 solid; padding: 15px; background-color: #ddefdd; font-size:100%; text-align:center">2. Read and Explain Dataset</div>"""

#Read data
data_original =  pd.read_csv("C:\\Users\\Mosta\\OneDrive\\Documents\\eslam\\credit_risk_dataset.csv")
data = pd.read_csv("C:\\Users\\Mosta\\OneDrive\\Documents\\eslam\\credit_risk_dataset.csv")

data

data.columns

"""### ğŸ§¾ Column Descriptions

- **person_age** >>> Ø¹Ù…Ø± Ø§Ù„Ù…ØªÙ‚Ø¯Ù‘ÙÙ… Ø¨Ø§Ù„Ø³Ù†ÙˆØ§Øª ÙˆÙ‚Øª Ø§Ù„ØªÙ‚Ø¯ÙŠÙ…  
- **person_income** >>> Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ø³Ù†ÙˆÙŠ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù„Ù„Ù…ØªÙ‚Ø¯Ù‘ÙÙ…  
- **person_home_ownership** >>> Ø­Ø§Ù„Ø© Ø³ÙƒÙ† Ø§Ù„Ù…ØªÙ‚Ø¯Ù‘ÙÙ… (Own/Rent/Mortgage/Other)  
- **person_emp_length** >>> Ù…Ø¯Ø© Ø§Ù„Ø®Ø¨Ø±Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ø§Ù„Ø³Ù†ÙˆØ§Øª  
- **loan_intent** >>> Ø§Ù„ØºØ±Ø¶ Ù…Ù† Ø§Ù„Ù‚Ø±Ø¶ (Ù…Ø«Ù„Ù‹Ø§: ØªØ¹Ù„ÙŠÙ…ØŒ Ø³ÙŠØ§Ø±Ø©ØŒ Ø£Ø¹Ù…Ø§Ù„...)  
- **loan_grade** >>> Ø¯Ø±Ø¬Ø©/ØªØµÙ†ÙŠÙ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù‚Ø±Ø¶ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ù‡Ø© Ø§Ù„Ù…Ù…ÙˆÙ‘ÙÙ„Ø© (Aâ€“G Ø¹Ø§Ø¯Ø©Ù‹)  
- **loan_amnt** >>> Ù‚ÙŠÙ…Ø© Ø§Ù„Ù‚Ø±Ø¶ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø£Ùˆ Ø§Ù„Ù…ØµØ±ÙˆÙØ©  
- **loan_int_rate** >>> Ù…Ø¹Ø¯Ù„ Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø³Ù†ÙˆÙŠ Ù„Ù„Ù‚Ø±Ø¶ (Ù†Ø³Ø¨Ø© Ù…Ø¦ÙˆÙŠØ©)  
- **loan_status** >>> Ø­Ø§Ù„Ø© Ø§Ù„Ù‡Ø¯Ù (Ø³Ø¯Ø§Ø¯ Ù†Ø§Ø¬Ø­ Ø£Ù… ØªØ¹Ø«Ù‘Ø± Ø¹Ù† Ø§Ù„Ø³Ø¯Ø§Ø¯)  
- **loan_percent_income** >>> Ù†Ø³Ø¨Ø© Ù‚ÙŠÙ…Ø© Ø§Ù„Ù‚Ø±Ø¶ Ø¥Ù„Ù‰ Ø¯Ø®Ù„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù‘ÙÙ… (loan_amnt / income)  
- **cb_person_default_on_file** >>> ÙˆØ¬ÙˆØ¯ ØªØ¹Ø«Ù‘Ø± Ø³Ø§Ø¨Ù‚ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø§Ø¦ØªÙ…Ø§Ù†ÙŠ (Y/N)  
- **cb_person_cred_hist_length** >>> Ø·ÙˆÙ„ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø§Ø¦ØªÙ…Ø§Ù†ÙŠ Ù„Ù„Ù…ØªÙ‚Ø¯Ù‘ÙÙ… (Ø¨Ø§Ù„Ø³Ù†ÙˆØ§Øª)
**bold text**

## <div style="border-radius:0px; border:#3eb489 solid; padding: 15px; background-color: #ddefdd; font-size:100%; text-align:center">3. EDA</div>
"""



data.head()

data.tail()

data.describe()

data.info()

data.isnull().sum()

data.nunique()

data.loan_status.value_counts()

data.duplicated().sum()

#  Ø§Ù„Ø§Ø±ØªØ¨Ø§Ø· ÙÙ‚Ø· Ø¨ÙŠÙ† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø±Ù‚Ù…ÙŠØ©
correlation_table = data.select_dtypes(include=[np.number]).corr()
# Ø§Ø®ØªØ§Ø± Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù„ÙŠ Ù†ÙˆØ¹Ù‡Ø§ Ø±Ù‚Ù…ÙŠ ÙÙ‚Ø·

print("Correlation matrix:\n")
print(correlation_table)

# person_age â†” cb_person_cred_hist_length â†’ 0.86   Ø¹Ù„Ø§Ù‚Ø© Ø·Ø±Ø¯ÙŠØ© Ù‚ÙˆÙŠØ© Ø¬Ø¯Ù‹Ø§
# loan_amnt â†” loan_percent_income â†’ 0.57        Ø¹Ù„Ø§Ù‚Ø© Ø·Ø±Ø¯ÙŠØ© Ù…ØªÙˆØ³Ø·Ø©
# loan_status â†” loan_int_rate â†’ 0.34              Ø¹Ù„Ø§Ù‚Ø© Ø·Ø±Ø¯ÙŠØ© Ù…ØªÙˆØ³Ø·Ø© (Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© Ø§Ù‚Ø±Ø¨ Ù„Ù„ØªØ¹Ø«Ø±)
# loan_status â†” loan_percent_income â†’ 0.38         Ø¹Ù„Ø§Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© (Ù†Ø³Ø¨Ø© Ø§Ù„Ù‚Ø±Ø¶ Ù„Ù„Ø¯Ø®Ù„ ØªØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¯Ø§Ø¯)
# person_income â†” loan_amnt â†’ 0.27                 Ø¹Ù„Ø§Ù‚Ø© Ø¶Ø¹ÙŠÙØ©

"""## Univarient Analysis"""

#MAX AND MIN AGE
#ØªØ­Ù„ÙŠÙ„ Ø¹Ù…ÙˆØ¯ person_age Ù„Ù…Ø¹Ø±ÙØ©:
# Ø£ÙƒØ¨Ø± ÙˆØ£ØµØºØ± Ø¹Ù…Ø± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§.

max_ = data['person_age'].max()
min_ = data['person_age'].min()
print(f"maximum Age {max_}")
print(f"minimum Age {min_}")
print("_"*70)
# ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù†Ø§Ø³ Ø¹Ù„Ù‰ ÙØ¦Ø§Øª Ø¹Ù…Ø±ÙŠØ© (Ù…Ù† 0â€“18ØŒ 19â€“26ØŒ 27â€“36 ÙˆÙ‡ÙƒØ°Ø§).

mx = int(data['person_age'].max())
bins = [0, 18, 26, 36, 46, 56, 66, mx]
labels = ["0-18", "19-26", "27-36", "37-46", "47-56", "57-66", f"67-{mx}"]

counts = pd.cut(data['person_age'],bins=bins,labels=labels,right=True,include_lowest=True).value_counts(sort=False)
print(counts)

 # ÙˆØ¬ÙˆØ¯ Ø¹Ù…Ø± 144 Ø³Ù†Ø© ØºÙŠØ± ÙˆØ§Ù‚Ø¹ÙŠ

s = data['person_age']
counts = pd.cut(s, bins=bins, labels=labels, include_lowest=True).value_counts(sort=False)
counts.plot(kind='bar'); plt.title('person_age (buckets)'); plt.ylabel('count'); plt.tight_layout(); plt.show()

#max and min income
max_ = data['person_income'].max()
min_ = data['person_income'].min()
print(f"maximum Income {max_}")
print(f"minimum Income {min_}")

 # np.inf >>> "Infinity" = Ø¹Ø¯Ø¯ Ù„Ø§ Ù†Ù‡Ø§Ø¦ÙŠ (âˆ) >>
 # ÙƒØ­Ø¯Ù‘ Ø£Ø¹Ù„Ù‰ Ù…ÙØªÙˆØ­ Ù„Ù„ÙØ¦Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©
# Ø¹Ø¯Ù‘ Ø£ØµØ­Ø§Ø¨ Ø§Ù„Ø¯Ø®Ù„ ØµÙØ± + (Ø§Ø®ØªÙŠØ§Ø±ÙŠ) Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø§Øª
inc_bins = [1, 4000, 10000, 25000, 50000, 75000, 100000, 150000, 250000,
            500000, 1000000, 2000000, 5000000, np.inf]

inc_counts = pd.cut(
    data.loc[data['person_income'] > 0, 'person_income'],
    bins=inc_bins,
    include_lowest=True
).value_counts(sort=False)

print(f"Income == 0: {(data['person_income'] == 0).sum()}")
for r, n in inc_counts.items():
    print(f"Income {r}: {n}")

#min and max loan amount
print(f"maximum Loan Amount {data['loan_amnt'].max()}")
print(f"minimum Loan Amount {data['loan_amnt'].min()}")

loan_bins = [1, 1000, 3000, 5000, 10000, 15000, 20000, 25000, 30000, 35000]
loan_counts = pd.cut(data['loan_amnt'],bins=loan_bins,include_lowest=True,right=True).value_counts(sort=False)
for r, n in loan_counts.items():
    print(f"Loan Amount {r}: Number of people {n}")

bins = [3000,5000,10000,15000,20000,25000,30000,35000]
vals, edges = np.histogram(data['loan_amnt'], bins=bins)
centers = 0.5*(edges[1:]+edges[:-1])

plt.bar(centers, vals, width=np.diff(edges), align='center')
plt.xticks(edges, [f"{int(b/1000)}k" for b in edges])
plt.xlabel('loan amount'); plt.ylabel('count'); plt.title('Loan amount (histogram)')
plt.tight_layout(); plt.show()

# Ù†ÙˆØ¹ Ø§Ù„Ø³ÙƒÙ† Ù„ÙƒÙ„ Ø¹Ù…ÙŠÙ„
# RENT â†’ Ø³Ø§ÙƒÙ† Ø¥ÙŠØ¬Ø§Ø±
# OWN â†’ Ø¹Ù†Ø¯Ù‡ Ø¨ÙŠØª Ù…Ù„Ùƒ
# MORTGAGE â†’ Ø´Ø§Ø±ÙŠ Ø¨ÙŠØª Ø¨Ù‚Ø±Ø¶ (ØªÙ…ÙˆÙŠÙ„ Ø¹Ù‚Ø§Ø±ÙŠ)
# OTHER â†’ Ù†ÙˆØ¹ Ø³ÙƒÙ† Ø¢Ø®Ø±

level_counts = data['person_home_ownership'].value_counts()
fig = px.bar(x=level_counts.index, y=level_counts.values,
             title='person_home_ownership', labels={'x':'category','y':'count'})
fig.update_layout(xaxis_tickangle=-30)
fig.show()

fig=px.histogram(data, x = 'loan_intent',histnorm = 'percent', text_auto = '.2f',template = 'presentation', title = 'loan intent',color_discrete_sequence=px.colors.sequential.Mint)
fig.update_layout()
fig.show()

#  Ø£Ø¹Ù…Ø¯Ø© ØªÙˆØ¶Ø­ ØªÙˆØ²ÙŠØ¹ Ø£ØºØ±Ø§Ø¶ Ø§Ù„Ù‚Ø±ÙˆØ¶ ÙƒÙ†ÙØ³ÙØ¨.

"""# <div style="border-radius:0px; border:#3eb489 solid; padding: 15px; background-color: #ddefdd; font-size:100%; text-align:center">4. Dealing with Outliers</div>"""

data.isnull().sum()

data['person_emp_length'].fillna(data['person_emp_length'].mean(), inplace=True)
data['loan_int_rate'].fillna(data['loan_int_rate'].mean(), inplace=True)

data.isnull().sum()

data['loan_status'].value_counts()

data.describe()

# Ø§Ø¹ØªØ¨Ø± Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø´Ø§Ø°Ø© NaN
data.loc[(data['person_age'] < 18) | (data['person_age'] > 100), 'person_age'] = np.nan
data.loc[(data['person_emp_length'] < 0) | (data['person_emp_length'] > 60), 'person_emp_length'] = np.nan

# ØªØ¹ÙˆÙŠØ¶ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø© Ø¨Ø§Ù„Ù…ØªÙˆØ³Ø·
data['person_age'].fillna(data['person_age'].mean(), inplace=True)
data['person_emp_length'].fillna(data['person_emp_length'].mean(), inplace=True)

# import matplotlib.pyplot as plt
# verti = data['person_age'].value_counts().values
# hori = data['person_age'].value_counts().index
# fig = plt.figure(figsize = (15, 5))
# plt.bar(hori, verti)
# # after 80 it is rare

"""# <div style="border-radius:0px; border:#3eb489 solid; padding: 15px; background-color: #ddefdd; font-size:100%; text-align:center">5. Feature Engineering</div>"""

data['age_group'] = pd.cut(data['person_age'],
                           bins=[20, 26, 36, 46, 56, 66],
                           labels=['20-25', '26-35', '36-45', '46-55', '56-65'])
# Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© :
# ÙƒÙˆÙ‘Ù† Ø¹Ù…ÙˆØ¯ Ø¬Ø¯ÙŠØ¯ Ø§Ø³Ù…Ù‡ age_group
# Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ‚Ø³ÙŠÙ… Ø¹Ù…Ø± Ø§Ù„Ø´Ø®Øµ (person_age) Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ø¹Ù…Ø±ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©

data['age_group'].dtype

data.head()

"""## **Income Group**"""

data['income_group'] = pd.cut(data['person_income'],
                              bins=[0, 25000, 50000, 75000, 100000, float('inf')],
                              labels=['low', 'low-middle', 'middle', 'high-middle', 'high'])

# Ù„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§ÙŠØ¶Ø§
# ÙƒÙˆÙ‘Ù† Ø¹Ù…ÙˆØ¯ Ø¬Ø¯ÙŠØ¯ Ø§Ø³Ù…Ù‡ income_group
# Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ‚Ø³ÙŠÙ… Ù…Ø±ØªØ¨ Ø§Ù„Ø´Ø®Øµ (person_income) Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø§Ù„ÙŠØ© Ù…Ø­Ø¯Ø¯Ø©

data.head()

"""## **Loan Amount**"""

data['loan_amount_group'] = pd.cut(data['loan_amnt'],
                                   bins=[0, 5000, 10000, 15000, float('inf')],
                                   labels=['small', 'medium', 'large', 'very large'])

# ÙƒÙˆÙ† Ø¹Ù…ÙˆØ¯ Ø¬Ø¯ÙŠØ¯ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù‚Ø±ÙˆØ¶

data['loan_amount_group']

data.head()

# Ø®Ù„Ù‚ Ø£Ø¹Ù…Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø© ØªÙ…Ø«Ù„ Ù†ÙØ³ÙØ¨ Ø¨ÙŠÙ† Ø¹Ù…ÙˆØ¯ÙŠÙ† Ø¹Ù„Ø´Ø§Ù† >> ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¨Ø¯Ù„ Ù…Ø§ ØªØ¨Øµ Ù„Ù„Ù…Ø¨Ù„Øº ÙˆØ§Ù„Ø¯Ø®Ù„ Ù…Ù†ÙØµÙ„ÙŠÙ† >> Ø§Ù„ØªÙ‚Ø§Ø· Ø¹Ù„Ø§Ù‚Ø§Øª ØºÙŠØ± Ø®Ø·ÙŠØ©
# Ø±Ø¨Ø· Ø¨ÙŠÙ† Ù…Ù‚Ø§Ø¯ÙŠØ± Ù…Ù‡Ù…Ø© Ø¨Ø¯Ù„ Ù…Ø§ ØªØ¨Øµ Ù„ÙƒÙ„ Ø¹Ù…ÙˆØ¯ Ù„ÙˆØ­Ø¯Ù‡

# Ù†Ø³Ø¨Ø© Ø§Ù„Ù‚Ø±Ø¶ Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ø®Ù„. ÙƒÙ„ Ù…Ø§ ØªÙƒØ¨Ø± Ø§Ù„Ù†Ø³Ø¨Ø©ØŒ Ø§Ù„Ø¹Ø¨Ø¡ Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù„Ù‚Ø±Ø¶ Ø£Ø¹Ù„Ù‰ >> ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø®Ø§Ø·Ø±Ø© Ø£Ø¹Ù„Ù‰ Ø¨Ø§Ù„ØªØ¹Ø«Ù‘Ø±.
# Create loan-to-income ratio
data['loan_to_income_ratio'] = data['loan_amnt'] / data['person_income']

# Ø·ÙˆÙ„ Ø§Ù„Ø®Ø¨Ø±Ø© Ù„ÙƒÙ„ ÙˆØ­Ø¯Ø© Ù…Ù† Ù…Ø¨Ù„Øº Ø§Ù„Ù‚Ø±Ø¶. Ø®Ø¨Ø±Ø© Ø£ÙƒØ¨Ø± Ù…Ù‚Ø§Ø¨Ù„ Ù‚Ø±Ø¶ Ø£ØµØºØ± ØªØ¹Ø·ÙŠ Ù‚ÙŠÙ…Ø© Ø£Ø¹Ù„Ù‰ (Ù‚Ø¯ ØªØ¹Ù†ÙŠ Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ù†Ø³Ø¨ÙŠ)
# Create loan-to-employment length ratio
data['loan_to_emp_length_ratio'] =  data['person_emp_length']/ data['loan_amnt']

# Ø³Ø¹Ø± Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ù„ÙƒÙ„ ÙˆØ­Ø¯Ø© Ù…Ù† Ù…Ø¨Ù„Øº Ø§Ù„Ù‚Ø±Ø¶
# Create interest rate-to-loan amount ratio
data['int_rate_to_loan_amt_ratio'] = data['loan_int_rate'] / data['loan_amnt']

raw_data = data.copy()

col_list = ['person_age',   # Features + Target
 'person_income',
 'person_home_ownership',
 'person_emp_length',
 'loan_intent',
 'loan_grade',
 'loan_amnt',
 'loan_int_rate',
 'loan_status',
 'loan_percent_income',
 'cb_person_default_on_file',
 'cb_person_cred_hist_length',
'age_group','income_group','loan_amount_group']

drop_colums = [] # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ Ø­Ø°ÙÙ‡Ø§ ØŒ Ø¨Ù†Ø­Ø· ÙÙŠÙ‡Ø§ Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© Ù‡ØªØ³ØªØ¨Ø¹Ø¯Ù‡Ø§
scale_cols = ['person_income','person_age','person_emp_length', 'loan_amnt','loan_int_rate','cb_person_cred_hist_length','loan_percent_income','loan_to_income_ratio', 'loan_to_emp_length_ratio',
       'int_rate_to_loan_amt_ratio']
# Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© Ø§Ù„Ù„ÙŠ Ù‡ØªØªØ¹Ù…Ù„Ù‘Ù‡Ø§ >>> Scaling (Standard/MinMa) >>> Ø²ÙŠ person_income, loan_amnt

ohe_colums = ['cb_person_default_on_file','loan_grade', 'person_home_ownership','loan_intent','income_group','age_group','loan_amount_group']
#Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù„ÙŠ Ù‡ØªØªØ¹Ù…Ù„Ù‘Ù‡Ø§ >> One-Hot Encoding >> Ø²ÙŠ loan_grade, person_home_ownership >> ÙˆÙƒÙ…Ø§Ù† Ø§Ù„Ø§Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ù‡ Ø²ÙŠ income_group, age_group, loan_amount_group

le_colums = [] # Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ùˆ Ø§Ø­ØªØ¬Øª Ø§Ø¹Ù…Ù„ Label Encoding
# Ø­Ø§Ù„ÙŠØ§ Ù…ÙÙŠØ´

data = data.drop(drop_colums, axis=1)

data.columns

"""# <div style="border-radius:0px; border:#3eb489 solid; padding: 15px; background-color: #ddefdd; font-size:100%; text-align:center">6. Data Preprocessing</div>

"""

# my target is "loan_status"
#

X = data.drop(['loan_status'], axis=1) # axis=1 ÙŠØ¹Ù†ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
Y = data['loan_status']

# splitting

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.2, random_state=12)

print(x_train.shape,x_test.shape)

x_train.reset_index(inplace = True)
x_test.reset_index(inplace = True)

x_train.columns

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder()
ohe.fit(x_train[ohe_colums])

ohe.categories_

merge_ohe_col = np.concatenate((ohe.categories_[0],
                ohe.categories_[1],
                ohe.categories_[2],
                ohe.categories_[3],
                ohe.categories_[4],
                ohe.categories_[5],
                ohe.categories_[6],))

#  ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø§Ù„ØªÙŠ ØªØ¹Ù„Ù‘Ù…Ù‡Ø§ Ø§Ù„Ù€
# OneHotEncoder
# Ù…Ù† Ø£ÙˆÙ„ 7 Ø£Ø¹Ù…Ø¯Ø© ÙØ¦ÙˆÙŠØ© ÙÙŠ Ù…ØµÙÙˆÙØ© ÙˆØ§Ø­Ø¯Ø©.

merge_ohe_col

# ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ§Ù† Ù‡ÙˆØª Ø§Ù†ÙƒÙˆØ¯Ø± Ø§Ø¹Ù…Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨
ohe_data = pd.DataFrame(ohe.transform(x_train[ohe_colums]).toarray(), columns=merge_ohe_col)
# ohe.transform >> ÙŠØ­ÙˆÙ‘Ù„Ù‡Ø§ Ù„Ù…ØµÙÙˆÙØ© One-Hot (0/1) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„ØªÙŠ ØªØ¹Ù„Ù‘Ù…Ù‡Ø§ ÙÙŠ fit
# toarray() >> Ø­ÙˆÙ‘Ù„ Ø§Ù„Ù†Ø§ØªØ¬ Ù…Ù† Ù…ØµÙÙˆÙØ© sparse matrix Ù…ØªÙ†Ø§Ø«Ø±Ù‡ Ø§Ùˆ Ù…Ø´ Ù…ØªØ¬Ù…Ø¹Ù‡ >>>
# Ø§Ù„ÙŠ  pd.DataFrame

# ØªØ¬Ù…ÙŠØ¹ Ø§Ù„ÙˆØ§Ù† Ù‡ÙˆØª Ø§Ù†ÙƒÙˆØ¯Ø± Ø§Ø¹Ù…Ø¯Ø© Ø§Ù„ØªÙŠØ³Øª
ohe_data2 = pd.DataFrame(ohe.transform(x_test[ohe_colums]).toarray(),columns=merge_ohe_col)

X_new = pd.concat([ohe_data, x_train], axis=1)
X_new = X_new.drop(ohe_colums, axis=1)
# Ø¶Ù… Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù€ One-Hot Ø§Ù„Ø¬Ø¯ÙŠØ¯Ù‡
# Ù…Ø¹ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø£ØµÙ„ÙŠØ©

X_new_test = pd.concat([ohe_data2, x_test], axis=1) # Ù†ÙØ³ Ø§Ù„ÙƒÙ„Ø§Ù… Ù„Ù„ØªÙŠØ³Øª
X_new_test = X_new_test.drop(ohe_colums, axis=1)

X_new.head()
X_new.shape

X_new_test.head()

# Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø³ÙƒÙ‘ÙŠÙ„Ø± Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„ÙƒÙ„ Ù†Ù…Ø· ØªÙˆØ²ÙŠØ¹.

from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler
# Ù…Ù†Ø§Ø³Ø¨ Ù„Ùˆ Ù…ÙÙŠØ´ Ø¢ÙˆØªÙ„Ø§ÙŠØ±Ø² Ù‚ÙˆÙŠØ© >> MinMaxScaler >>> ÙŠØ¹ÙŠØ¯ Ù‚ÙŠØ§Ø³ Ø§Ù„Ù‚ÙŠÙ… Ø¥Ù„Ù‰ Ù…Ø¬Ø§Ù„ Ø«Ø§Ø¨Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§ [0, 1]).
# StandardScaler >> ÙŠØ±ÙƒØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ù…ØªÙˆØ³Ø· ÙˆÙŠØ¬Ø¹Ù„ Ø§Ù„Ø§Ù†Ø­Ø±Ø§Ù Ø§Ù„Ù…Ø¹ÙŠØ§Ø±ÙŠ â‰ˆ 1
# RobustScaler >> ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ¯ÙŠØ§Ù† Ùˆ IQR >> ÙØ§ Ø¨ÙŠØªØ¹Ø§Ù…Ù„ Ø§Ø­Ø³Ù† Ù…Ø¹ Ø§Ù„Ø§ÙˆØªÙ„Ø§ÙŠØ±Ø²

scale_cols = ['person_income','person_age','person_emp_length', 'loan_amnt','loan_int_rate','cb_person_cred_hist_length','loan_percent_income', 'loan_to_emp_length_ratio',
       'int_rate_to_loan_amt_ratio']
       # Ù‚Ø§Ø¦Ù…Ø© Ø¹Ø§Ù…Ø© Ø¨Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ Ø¹Ù…Ù„ Ø³ÙƒÙŠÙ„ÙŠÙ†Ø¬ Ù„Ù‡Ø§

uniform_col= []

normal_col = ['person_income','person_age','person_emp_length', 'loan_amnt','loan_int_rate','cb_person_cred_hist_length','loan_percent_income', 'loan_to_emp_length_ratio',
       'int_rate_to_loan_amt_ratio']

bimodal_col = [] # Ù„ÙŠØ³Øª Ù„Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù„ÙŠ ØªÙˆØ²ÙŠØ¹Ù‡Ø§ ÙÙŠÙ‡ Ø¢ÙˆØªÙ„Ø§ÙŠØ±Ø² Ù‚ÙˆÙŠØ© Ù‡ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø¨Ø¹Ø¯ÙŠÙ†


scaler_uniform = MinMaxScaler()
#X_new.loc[:,uniform_col] = scaler_uniform.fit_transform(X_new.loc[:,uniform_col])
#X_new_test.loc[:,uniform_col] = scaler_uniform.transform(X_new_test.loc[:,uniform_col])

scaler_normal = StandardScaler()
X_new.loc[:,normal_col] = scaler_normal.fit_transform(X_new.loc[:,normal_col])
X_new_test.loc[:,normal_col] = scaler_normal.transform(X_new_test.loc[:,normal_col])

scaler_bimodal = RobustScaler()
#X_new.loc[:,bimodal_col] = scaler_bimodal.fit_transform(X_new.loc[:,bimodal_col])
#X_new_test.loc[:,bimodal_col] = scaler_bimodal.transform(X_new_test.loc[:,bimodal_col])

X_new_test.head()

"""# <div style="border-radius:0px; border:#3eb489 solid; padding: 15px; background-color: #ddefdd; font-size:100%; text-align:center">7. ML Classification Models</div>

"""


# 1) Classification Models

from sklearn.svm import SVC  # ÙŠÙØµÙ„ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ø¨Ø£ÙƒØ¨Ø± Ù‡Ø§Ù…Ø´ Ùˆ ÙƒÙˆÙŠØ³ Ø§ÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ù„ÙŠ Ù„ÙŠÙ‡Ø§ ÙÙŠØªØ´Ø±Ø² ØµØºÙŠØ±Ø©/Ù…ØªÙˆØ³Ø·Ø©

from sklearn.naive_bayes import MultinomialNB  # Ù…ØµÙ†Ù‘Ù Ø¨Ø§ÙŠØ² Ù…Ù†Ø§Ø³Ø¨ Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¯Ù‘/Counts ØºÙŠØ± Ø³Ø§Ù„Ø¨Ø© (Ù†ØµÙˆØµ/TF)

from sklearn.tree import DecisionTreeClassifier  # Ø´Ø¬Ø±Ø© Ù‚Ø±Ø§Ø± Ù„Ù„ØªØµÙ†ÙŠÙØ› ØªÙ„ØªÙ‚Ø· Ø§Ù„Ù„Ø§Ø®Ø·Ù‘ÙŠØ© ÙˆØ§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ø¨Ø¨Ø³Ø§Ø·Ø©

from sklearn.neighbors import KNeighborsClassifier  # Ù…ØµÙ†Ù‘Ù Ø£Ù‚Ø±Ø¨ Ø§Ù„Ø¬ÙŠØ±Ø§Ù†Ø› ÙŠØ­ØªØ§Ø¬ Scaling ÙˆØºØ§Ù„ÙŠ Ø­Ø³Ø§Ø¨ÙŠÙ‹Ø§ ÙˆÙ‚Øª Ø§Ù„ØªØ³Øª

from sklearn.ensemble import RandomForestClassifier  # ØºØ§Ø¨Ø© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù‚ÙˆÙŠØ© ÙˆØ«Ø§Ø¨ØªØ©Ø› ØªÙ‚Ù„ÙŠÙ„ ØªØ¨Ø§ÙŠÙ† + Ø£Ù‡Ù…ÙŠØ© Ø®ØµØ§Ø¦Øµ

from sklearn.ensemble import AdaBoostClassifier  # Ù…ÙÙŠØ¯ Ø¹Ù„Ù‰ Ø¯Ø§ØªØ§ Ù†Ø¸ÙŠÙØ© ÙˆØ­Ø¯ÙˆØ¯ ÙˆØ§Ø¶Ø­Ø© (Boosting Ø¨Ù…ØªØ¹Ù„Ù…ÙŠÙ† Ø¶Ø¹ÙØ§Ø¡)

from sklearn.ensemble import BaggingClassifier  # Ù…Ø¬Ù…Ù‘Ø¹ Bagging Ù„Ù„ØªØµÙ†ÙŠÙ ÙŠÙ‚Ù„Ù‘Ù„ Ø§Ù„ØªØ¨Ø§ÙŠÙ† Ø¹Ø¨Ø± Bootstrap

from sklearn.ensemble import ExtraTreesClassifier  # Ø£Ø´Ø¬Ø§Ø± Ø´Ø¯ÙŠØ¯Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©Ø› Ø³Ø±ÙŠØ¹Ø© ÙˆØ£Ø­ÙŠØ§Ù†Ù‹Ø§ Ø£Ù‚Ù„ ØªØ¨Ø§ÙŠÙ†

from sklearn.ensemble import GradientBoostingClassifier  # Gradient Boosting Ù„Ù„ØªØµÙ†ÙŠÙ (GBDT Ø³ÙƒÙØªÙ„ÙŠØ±Ù†)

from xgboost import XGBClassifier  # Ù…ØµÙ†Ù‘Ù XGBoost Ù‚ÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆÙŠØ¯Ø¹Ù… Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù† Ø¨Ø§Ù„ÙØ¦Ø©

from sklearn.linear_model import LogisticRegression  # Ù…ØµÙ†Ù‘Ù Ù„ÙˆØ¬Ø³ØªÙŠ Ø®Ø·Ù‘ÙŠ Ø³Ø±ÙŠØ¹ ÙˆÙŠØ·Ù„Ø¹ Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ÙˆØ§Ø¶Ø­Ø©

from catboost import CatBoostClassifier  # Ù…Ù…ØªØ§Ø² Ù…Ø¹ (Categoricals) Ù…Ø¨Ø§Ø´Ø±Ø©Ù‹

import lightgbm as lgb
from lightgbm import LGBMClassifier  # LightGBM Ù„Ù„ØªØµÙ†ÙŠÙ Ø³Ø±ÙŠØ¹ ÙˆÙØ¹Ù‘Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ø¬Ø¯ÙˆÙ„ÙŠØ©

# 2) Regression Models
# from sklearn.linear_model import LinearRegression  # Ø§Ù†Ø­Ø¯Ø§Ø± Ø®Ø·Ù‘ÙŠ Ù„Ù…Ù‡Ø§Ù… Regression (Ù…Ø´ ØªØµÙ†ÙŠÙ)
# from lightgbm import LGBMRegressor                 # LightGBM Ù„Ù„Ø§Ù†Ø­Ø¯Ø§Ø± (Regression)

# 3) Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù‚ÙŠØ§Ø³
from sklearn.metrics import make_scorer  # Ø¨Ù†Ø§Ø¡ Scorer Ù…Ø®ØµÙ‘Øµ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Grid/CV

# from sklearn.metrics import mean_squared_error as MSE  # Ù…Ù‚ÙŠØ§Ø³ Ù„Ù„Ù€ Regression (Ù…Ø´ Ù„Ù„ØªØµÙ†ÙŠÙ)
# from sklearn.metrics import mean_absolute_error  # Ù…Ù‚ÙŠØ§Ø³ Ù„Ù„Ù€ Regression (Ù…Ø´ Ù„Ù„ØªØµÙ†ÙŠÙ)

# 4) Ø¶Ø¨Ø· Ø§Ù„Ù‡Ø§ÙŠØ¨Ø± Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø² (Optimization)
from hyperopt import hp, fmin, tpe  # Ø£ÙØ¶Ù„ Ù‚ÙŠÙ… Ù„Ù„Ø³ÙˆØ¨Ø±-Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø² (Ø²ÙŠ n_estimators, max_depth, learning_rate ÙÙŠ XGBoost). Ø¨Ø¯Ù„ Ù…Ø§ Ù†Ø¬Ø±Ù‘Ø¨ ÙŠØ¯ÙˆÙŠ
from bayes_opt import BayesianOptimization  # Ù„Ø¶Ø¨Ø· Ù‡Ø§ÙŠØ¨Ø± Ø¨Ø§Ø±Ø§Ù…ÙŠØªØ±Ø² Ù†Ù…Ø§Ø°Ø¬ Ù…Ø«Ù„ LightGBM/XGBoost/CatBoost Ø¨Ø³Ø±Ø¹Ø© ÙˆÙƒÙØ§Ø¡Ø©.

# 5) ØªÙ‚Ø³ÙŠÙ…Ø§Øª ÙˆØªÙ‚ÙŠÙŠÙ… Ø¨Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ (CV)
from sklearn.model_selection import GridSearchCV, StratifiedKFold
# GridSearchCV: Ø¨Ø­Ø« Ø´Ø¨ÙƒÙŠ Ù…Ù†Ø¸Ù‘Ù… Ø¹Ù„Ù‰ Ù‚ÙŠÙ… Ù…Ø­Ø¯Ù‘Ø¯Ø©. >>>>> StratifiedKFold: ØªÙ‚Ø³ÙŠÙ… K-Fold ÙŠØ­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ù†Ø³Ø¨Ø© Ø§Ù„ÙØ¦Ø§Øª Ù…Ù‡Ù… Ø¬Ø¯Ù‹Ø§ Ù…Ø¹ Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù†

from sklearn.model_selection import KFold, cross_val_score  # KFold Ø¨Ø¯ÙˆÙ† Ø­ÙØ¸ Ù†Ø³Ø¨ Ø§Ù„ÙØ¦Ø§Øª + Ø­Ø³Ø§Ø¨ Ù†ØªØ§Ø¦Ø¬ CV Ø¨Ø³Ø±Ø¹Ø©

#####################

from sklearn.linear_model import LinearRegression
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error as MSE

svc = SVC()
knc = KNeighborsClassifier() #algorithm='ball_tree', leaf_size=10, n_neighbors=18, p=1, weights='distance'
mnb = MultinomialNB()
dtc = DecisionTreeClassifier()
lrc = LogisticRegression()
rfc = RandomForestClassifier()
abc = AdaBoostClassifier()
bc = BaggingClassifier()
etc = ExtraTreesClassifier()
gbdt = GradientBoostingClassifier()
xgb = XGBClassifier()
cat = CatBoostClassifier(verbose=0)
lgb = lgb.LGBMClassifier()

svc = SVC()
knc = KNeighborsClassifier()  # algorithm='ball_tree', leaf_size=10, n_neighbors=18, p=1, weights='distance'

mnb = MultinomialNB()  # Ù…ØµÙ†Ù‘Ù Ø¨Ø§ÙŠØ²; ÙŠØªÙˆÙ‚Ù‘Ø¹ Ù…Ø¯Ø®Ù„Ø§Øª ØºÙŠØ± Ø³Ø§Ù„Ø¨Ø© (counts/TF)ØŒ Ù…Ø´ Ù…Ù‚ÙŠÙ‘Ø³Ø© Ø¨Ù‚ÙŠÙ… Ø³Ø§Ù„Ø¨Ø©

dtc = DecisionTreeClassifier()  # Ø´Ø¬Ø±Ø© Ù‚Ø±Ø§Ø± Ù„Ù„ØªØµÙ†ÙŠÙ

lrc = LogisticRegression()
rfc = RandomForestClassifier()
abc = AdaBoostClassifier()
bc  = BaggingClassifier()
etc = ExtraTreesClassifier()  # Ø£Ø´Ø¬Ø§Ø± Ø´Ø¯ÙŠØ¯Ø© Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©Ø› Ø³Ø±ÙŠØ¹Ø© ØºØ§Ù„Ø¨Ù‹Ø§

gbdt = GradientBoostingClassifier()
xgb = XGBClassifier()
cat = CatBoostClassifier()
from lightgbm import LGBMClassifier
lgb_clf = LGBMClassifier()  # ØªØ¨Ù†ÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø±Ø§Ø­Ù„ Ùˆ Ø¨ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø£Ø´Ø¬Ø§Ø± Ø§Ù„Ù‚Ø±Ø§Ø± Ø¨Ù‡Ø¯Ù Ø§Ù„ØªØµÙ†ÙŠÙ Ø¨Ø³Ø±Ø¹Ø© ÙˆÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ©

# Ø¬Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ÙÙŠ Ù‚Ø§Ù…ÙˆØ³ Ø¹Ø´Ø§Ù† Ù†Ø´ØºÙ‘Ù„ØŒ Ù†Ù‚Ø§Ø±Ù†ØŒ ÙˆÙ†Ø¯ÙŠØ± Ù†ØªØ§Ø¦Ø¬ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª ÙƒØªÙŠØ±

clf = {
    'RF': rfc,
    'LR': lrc,
    'DT': dtc,
    'KN' : knc,
    'xgb':xgb,
    'cat':cat,
}

# clfs = {
#     'SVC' : svc,
#     'KN' : knc,
#     'NB': mnb,
#     'DT': dtc,
#     'LR': lrc,
#     'RF': rfc,
#     'AdaBoost': abc,
#     'BgC': bc,
#     'ETC': etc,
#     'GBDT':gbdt,
#     'xgb':xgb
# }

from sklearn.metrics import precision_score, accuracy_score, recall_score, confusion_matrix

def train_classifier(clf, X_train, y_train, X_test, y_test):
    clf.fit(X_train, y_train)  # ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨
    y_pred = clf.predict(X_test) # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)

    # Ø­Ø³Ø§Ø¨ CM to get >>> TN and FP
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()  # ravel() >>> ÙŠÙØªØ±Ø¶ Ù…ØµÙÙˆÙØ© 2Ã—2
    specificity = tn / (tn + fp)
    # Ù†Ø³Ø¨Ø© Ø§Ù„Ø³Ø§Ù„Ø¨ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø§Ù„Ù„ÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§ÙƒØªØ´ÙÙ‡ ØµØ­ Ù…Ù† ÙƒÙ„ Ø§Ù„Ø³ÙˆØ§Ù„Ø¨ Ø§Ù„ÙØ¹Ù„ÙŠØ©.

    return accuracy, precision, recall, specificity
# -_- -_- -_- -_- -_- -_- -_- -_- -_- -_- -_- #

# Precision = TP / (TP + FP)
# Recall  = TP / (TP + FN)

# -_- -_- -_- -_- -_- -_- -_- -_- -_- -_- -_- #

# Usage
# accuracy, precision, recall, specificity = train_classifier(clf, x_train, y_train, X_test, y_test)
# print("Accuracy:", accuracy)
# print("Precision:", precision)
# print("Recall:", recall)
# print("Specificity:", specificity)

X_new = X_new.drop(columns=[col for col in X_new.columns if pd.isna(col)], axis=1)
X_new_test = X_new_test.drop(columns=[col for col in X_new_test.columns if pd.isna(col)], axis=1)

accuracy_scores = []
precision_scores = []
recall_scores = []
specificity_scores = []

for name,clf in clf.items():

    current_accuracy,current_precision, current_recall, current_specificity = train_classifier(clf, X_new,y_train,X_new_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)
    print("Recall - ",current_recall)
    print("Specificity - ",current_specificity)
    print()

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)
    recall_scores.append(current_recall)
    specificity_scores.append(current_specificity)

# DT >>> Recall -  0.75

# XGB >> Recall -  0.77 >> Ø§Ù„Ø§Ù†Ø³Ø¨ Ø¹Ø´Ø§Ù† :
# â€œØ§Ù„Ù…ÙˆØ¯ÙŠÙ„ ÙŠØªÙˆÙ‚Ø¹ Ø¥Ù† ÙÙ„Ø§Ù† Ù‡ÙŠØªØ®Ù„Ù Ø¨Ø³ Ù‡Ùˆ Ø£ØµÙ„Ù‹Ø§ Ù…Ø´ Ù‡ÙŠØªØ®Ù„Ùâ€ = False Positive
# Ù…Ø³ØªØ¹Ø¯ Ø£Ø¶Ø­Ù‘ÙŠ Ø¨Ø´ÙˆÙŠØ© Ø¹Ù…Ù„Ø§Ø¡ ÙƒÙˆÙŠØ³ÙŠÙ† (FP)
# Ù…ÙŠØ²ÙŠØ¯Ø´ Ø¹Ù†Ø¯ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ù„ÙŠ Ù‡ÙŠØªØ®Ù„ÙÙˆØ§ Ø¹Ù† Ø§Ù„Ø³Ø¯Ø§Ø¯

# cat >> Recall -  0.74

"""# <div style="border-radius:0px; border:#3eb489 solid; padding: 15px; background-color: #ddefdd; font-size:100%; text-align:center">8. Hyperparameter Tuning</div>

> Add blockquote


"""

# Ø§Ù„Ø§ØµÙ„ÙŠ
# # Hyperparameter-tuning: Bayesian Optimization, bayes_opt
# def lgbm_clf_bo(num_leaves, max_depth, learning_rate, min_child_weight, subsample, colsample_bytree):
#     params_lgbm = {'objective': 'binary'}
#     params_lgbm['num_leaves'] = round((2**round(max_depth))*num_leaves)
#     params_lgbm['max_depth'] = round(max_depth)
#     params_lgbm['learning_rate'] = learning_rate
#     params_lgbm['min_child_weight'] = min_child_weight
#     params_lgbm['subsample'] = subsample
#     params_lgbm['colsample_bytree'] = colsample_bytree

#     scores = cross_val_score(LGBMClassifier(random_state=12, **params_lgbm),
#                              X_new, y_train, scoring='recall', cv=10).mean()
#     return scores

# # Set parameters distribution
# params_lgbm ={
#     'num_leaves':(0.5,0.9),
#     'max_depth': (3, 15),
#     'learning_rate': (0.005, 0.3),
#     'min_child_weight':(1e-6, 1e-1),
#     'subsample':(0.5, 1),
#     'colsample_bytree':(0.5, 1)
# }

# # Run Bayesian Optimization
# lgbm_bo = BayesianOptimization(lgbm_clf_bo, params_lgbm)
# lgbm_bo.maximize(init_points=2, n_iter=20)

# Hyperparameter Tuning for XGBoost (Bayesian Optimization)

# from bayes_opt import BayesianOptimization
# from xgboost import XGBClassifier
# from sklearn.model_selection import StratifiedKFold, cross_val_score
# import numpy as np

# Ø§Ø­Ø³Ø¨ Ù†Ø³Ø¨Ø© Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù†: scale_pos_weight â‰ˆ (#negative / #positive)
pos = int(np.sum(y_train))
neg = len(y_train) - pos
base_spw = neg / max(1, pos)

def xgb_clf_bo(max_depth, learning_rate, subsample, colsample_bytree,
               min_child_weight, gamma, reg_lambda, reg_alpha,
               n_estimators, spw_mult):
    params = {
        "objective": "binary:logistic",
        "eval_metric": "logloss",        # Ø§Ù„Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„Ø› Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠ Ø¨Ø§Ù„Ù€ recall Ø¹Ø¨Ø± CV
        "tree_method": "hist",
        "n_jobs": -1,
        "random_state": 12,

        "max_depth": int(round(max_depth)),
        "learning_rate": learning_rate,
        "subsample": subsample,
        "colsample_bytree": colsample_bytree,
        "min_child_weight": max(1.0, min_child_weight),
        "gamma": gamma,
        "reg_lambda": reg_lambda,
        "reg_alpha": reg_alpha,
        "n_estimators": int(round(n_estimators)),
        "scale_pos_weight": base_spw * spw_mult
    }

    model = XGBClassifier(**params)
    # CV Ø®ÙÙŠÙ Ù„Ù„ØªØ¬Ø±ÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„ÙŠØ› ØªÙ‚Ø¯Ù‘Ø± ØªØ²ÙˆØ¯Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=12)
    score = cross_val_score(model, X_new, y_train, scoring="recall", cv=cv, n_jobs=-1).mean()
    return score

# Ø­Ø¯ÙˆØ¯ Ø¨Ø­Ø« ÙˆØ§Ù‚Ø¹ÙŠØ©
pbounds = {
    "max_depth": (3, 10),
    "learning_rate": (0.01, 0.2),
    "subsample": (0.6, 1.0),
    "colsample_bytree": (0.6, 1.0),
    "min_child_weight": (1.0, 50.0),
    "gamma": (0.0, 5.0),
    "reg_lambda": (0.0, 5.0),
    "reg_alpha": (0.0, 1.5),
    "n_estimators": (300, 1500),
    "spw_mult": (0.6, 1.6)  # Ù†Ø³Ù…Ø­ Ø¨Ø§Ù„ØªØ­Ø±ÙŠÙƒ Ø­ÙˆØ§Ù„ÙŠÙ† Ø§Ù„Ù†Ø³Ø¨Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
}

xgb_bo = BayesianOptimization(f=xgb_clf_bo, pbounds=pbounds, random_state=12, verbose=2)
xgb_bo.maximize(init_points=3, n_iter=25)

# Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© (Recall) ÙˆØ£ÙØ¶Ù„ Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø²:
# xgb_bo.max

xgb_bo.max

# Best hyperparameters â€” XGBoost (Ù…ÙƒØ§ÙØ¦ Ù„Ø®Ø·ÙˆØ© LightGBM ÙÙŠ Ø§Ù„ØµÙˆØ±Ø©)
import numpy as np

# Ù†Ø³Ø¨Ø© Ø¹Ø¯Ù… Ø§Ù„ØªÙˆØ§Ø²Ù† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„ØªØ­ÙˆÙŠÙ„ spw_mult Ø¥Ù„Ù‰ scale_pos_weight
pos = int(np.sum(y_train))
neg = len(y_train) - pos
base_spw = neg / max(1, pos)

best = xgb_bo.max['params'].copy()

best_xgb_params = {
    'max_depth'       : int(round(best['max_depth'])),
    'learning_rate'   : float(best['learning_rate']),
    'subsample'       : float(best['subsample']),
    'colsample_bytree': float(best['colsample_bytree']),
    'min_child_weight': max(1.0, float(best['min_child_weight'])),
    'gamma'           : float(best['gamma']),
    'reg_lambda'      : float(best['reg_lambda']),
    'reg_alpha'       : float(best['reg_alpha']),
    'n_estimators'    : int(round(best['n_estimators'])),
    'scale_pos_weight': base_spw * float(best['spw_mult']),
}

best_xgb_params  # ÙŠØ·Ø¨Ø¹ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©

# Feature Importance (Classification) â€” RandomForestClassifier
# Ø§Ù„ØºØ±Ø¶ = ØªÙØ³ÙŠØ± ÙˆØªØ±ØªÙŠØ¨ Ø§Ù„ÙÙŠØªØ´Ø±Ø²

rfc.fit(X_new, y_train)
feature_scores = pd.Series(rfc.feature_importances_, index=X_new.columns).sort_values(ascending=False)
feature_scores

# Ø§Ø¹Ù„ÙŠ ÙÙŠØªØ´Ø±Ø² ØªØ£Ø«ÙŠØ± ÙÙŠ Ø§Ù„Ù†ØªØ§ÙŠØ¬
# loan_to_income_ratio	0.125562
# loan_percent_income	0.113448
# person_income	0.089625
# loan_int_rate	0.088775

# ===== Build best_xgb_params from xgb_bo.max =====
import numpy as np

assert 'xgb_bo' in globals(), "Ù„Ø§Ø²Ù… xgb_bo ÙŠÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯ (Ø®Ù„ÙŠÙ‘Ø© Ø§Ù„Ù€ BO Ù…ØªØ´ØºÙ‘Ù„Ø©)."
assert 'y_train' in globals(), "Ù„Ø§Ø²Ù… y_train ÙŠÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯."

best = xgb_bo.max['params']

pos = int(np.sum(y_train))
neg = len(y_train) - pos
base_spw = neg / max(1, pos)

best_xgb_params = {
    'max_depth'       : int(round(best['max_depth'])),
    'learning_rate'   : float(best['learning_rate']),
    'subsample'       : float(best['subsample']),
    'colsample_bytree': float(best['colsample_bytree']),
    'min_child_weight': float(best['min_child_weight']),
    'gamma'           : float(best['gamma']),
    'reg_lambda'      : float(best['reg_lambda']),
    'reg_alpha'       : float(best['reg_alpha']),
    'n_estimators'    : int(round(best['n_estimators'])),
    'scale_pos_weight': base_spw * float(best['spw_mult']),
}

print("âœ… best_xgb_params Ø¬Ø§Ù‡Ø²Ø©.")
best_xgb_params

"""**Testing**"""

# ===== Group final features into a dictionary (auto from X_new) =====
import json
assert 'X_new' in globals(), "X_new Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯."

feat_cols = list(X_new.columns)
present = set(feat_cols)

# ØªØ±ØªÙŠØ¨ Ù…Ù†Ø·Ù‚ÙŠ Ù„Ù„ÙØ¦Ø§Øª
groups = {
    "Ø¯Ø±Ø¬Ø© Ø§Ø¦ØªÙ…Ø§Ù†ÙŠØ©":        ["A","B","C","D","E","F","G"],
    "ÙˆØ¶Ø¹ Ø³ÙƒÙ†":             ["MORTGAGE","RENT","OWN","OTHER"],
    "ØºØ±Ø¶ Ø§Ù„Ù‚Ø±Ø¶":           ["DEBTCONSOLIDATION","EDUCATION","HOMEIMPROVEMENT","MEDICAL","PERSONAL","VENTURE"],
    "Ø´Ø±ÙŠØ­Ø©/Ø¨Ø§Ù†Ø¯":          ["high","high-middle","middle","low-middle","low"],
    "ÙØ¦Ø© Ø¹Ù…Ø±ÙŠØ©":           ["20-25","26-35","36-45","46-55","56-65"],
    "Ø­Ø¬Ù…/Ø¨Ø§Ù†Ø¯":            ["small","medium","large","very large"],
    "Ø«Ù†Ø§Ø¦ÙŠ":               ["N","Y"],
}

grouped = {}
for k, cols in groups.items():
    subset = [c for c in cols if c in present]
    if subset:
        grouped[k] = subset

# Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© Ø£Ø®Ø±Ù‰ (Ø¹Ø¯Ø¯ÙŠØ©/Ù…Ø´ØªÙ‚Ø©) ØªØªØ¬Ù…Ø¹ Ù‡Ù†Ø§ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§
others = [c for c in feat_cols if all(c not in cols for cols in groups.values())]
if others:
    grouped["Ø®ØµØ§Ø¦Øµ Ø±Ù‚Ù…ÙŠØ©/Ù…Ø´ØªÙ‚Ø©"] = others

print(json.dumps(grouped, ensure_ascii=False, indent=2))

# ===== Bank-style Test with RAW-friendly display =====
import numpy as np, pandas as pd
from sklearn.utils.validation import check_is_fitted
from xgboost import XGBClassifier

assert all(v in globals() for v in ['X_new_test','y_test','X_new', 'best_xgb_params']), "Ù„Ø§Ø²Ù… X_new, X_new_test, y_test, and best_xgb_params be available."

# 1) Select the best performing model (XGBoost) and fit it
_model = XGBClassifier(**best_xgb_params)
_model.fit(X_new, y_train) # Fit the model

# 2) Ø§Ø®ØªØ§Ø± Ø¹ÙŠÙ‘Ù†Ø©
idx = 0  # â† ØºÙŠÙ‘Ø±Ù‡Ø§ Ù„ØªØ¬Ø±Ø¨Ø© Ø¹Ù…ÙŠÙ„ ØªØ§Ù†ÙŠ
row_std = X_new_test.iloc[[idx]]
y_true = int(np.array(y_test)[idx])

# 3) ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª (Ù„Ø¹Ø±Ø¶ Ø§Ù„ÙØ¦Ø§Øª ÙƒØ¨Ø´Ø±)
groups = {
    "Grade":          ["A","B","C","D","E","F","G"],
    "Residence":      ["MORTGAGE","RENT","OWN","OTHER"],
    "Purpose":        ["DEBTCONSOLIDATION","EDUCATION","HOMEIMPROVEMENT","MEDICAL","PERSONAL","VENTURE"],
    "Age band":       ["20-25","26-35","36-45","46-55","56-65"],
    "Size band":      ["small","medium","large","very large"],
    "Binary flag":    ["Y","N"],
    "Band":           ["high","high-middle","middle","low-middle","low"],
}
def pick_onehot(r, cols):
    present = [c for c in cols if c in r.columns]
    if not present: return "(n/a)"
    vals = r[present].iloc[0].to_dict()
    best = max(present, key=lambda c: vals[c])
    return best if vals[best] > 0.5 else "(baseline/none)"

# 4) Ù†Ø­Ø§ÙˆÙ„ Ù†Ø¬ÙŠØ¨ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø®Ø§Ù… (RAW) ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§:
raw_numeric_cols = [
    "person_age","person_income","person_emp_length","loan_amnt","loan_int_rate",
    "loan_percent_income","cb_person_cred_hist_length","loan_to_income_ratio",
    "int_rate_to_loan_amt_ratio","loan_to_emp_length_ratio"
]

def looks_standardized(df, cols):
    s = df[cols].select_dtypes(include='number')
    if s.empty: return False
    mu = s.mean().abs().mean()
    sigma = s.std(ddof=0).mean()
    return (mu < 0.5) and (abs(sigma - 1) < 0.5)

def find_raw_df(index_label):
    cands = []
    for name, obj in globals().items():
        if isinstance(obj, pd.DataFrame) and all(c in obj.columns for c in raw_numeric_cols):
            if index_label in obj.index:
                cands.append((name, obj))
    # Ø§Ø³ØªØ¨Ø¹Ø¯ Ø§Ù„Ø¯Ø§ØªØ§ Ø§Ù„Ù…Ù‚ÙŠÙ‘Ø³Ø© (Ø²ÙŠ X_new / X_new_test)
    for name, df in cands:
        try:
            if not looks_standardized(df, raw_numeric_cols):
                return df
        except Exception:
            pass
    return None

def inverse_with_scaler():
    # Ù†Ø­Ø§ÙˆÙ„ Ù†Ù„Ø§Ù‚ÙŠ StandardScaler Ù…ØªØ®Ø²Ù†
    scaler = None
    for obj in globals().values():
        if hasattr(obj, "mean_") and hasattr(obj, "scale_") and hasattr(obj, "feature_names_in_"):
            scaler = obj
            break
    if scaler is None: return None
    raw = {}
    for c in raw_numeric_cols:
        if c in row_std.columns and c in scaler.feature_names_in_:
            j = list(scaler.feature_names_in_).index(c)
            z = float(row_std[c].iloc[0])
            raw[c] = z * float(scaler.scale_[j]) + float(scaler.mean_[j])
    return raw if raw else None

index_label = row_std.index[0]
raw_df = find_raw_df(index_label)
raw_vals = None
if raw_df is not None:
    # Ø³Ø­Ø¨ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø®Ø§Ù… Ù…Ø¨Ø§Ø´Ø±Ø©
    raw_vals = {c: float(raw_df.loc[index_label, c]) for c in raw_numeric_cols if c in raw_df.columns}
else:
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¹ÙƒØ³ Ø§Ù„Ù‚ÙŠØ§Ø³ Ø¹Ø¨Ø± StandardScaler Ù„Ùˆ Ù…ØªÙˆÙØ±
    raw_vals = inverse_with_scaler()

# 5) ØªÙ†Ø¨Ø¤
def predict_score(model, X):
    if hasattr(model, "predict_proba"):
        return float(model.predict_proba(X)[:, -1][0])
    if hasattr(model, "decision_function"):
        s = float(model.decision_function(X)[0]);  return 1.0 / (1.0 + np.exp(-s))
    return float(model.predict(X)[0])

check_is_fitted(_model)
score = predict_score(_model, row_std)
pred  = int(score >= 0.5)
lbl   = {0:"its safe", 1:"High Risk"}[pred]
actual= {0:"its safe", 1:"High Risk"}[y_true]
ok    = "âœ… Correct" if pred == y_true else "âŒ Incorrect"

# 6) Ø¹Ø±Ø¶ Ù…Ù†Ø³Ù‘Ù‚
print(f"ğŸ¦ Test case idx: {idx}")
print("â€” Selected categories â€”")
for k, cols in groups.items():
    print(f"  {k}: {pick_onehot(row_std, cols)}")

print("\nâ€” Customer values (RAW view) â€”")
if raw_vals:
    for k in raw_numeric_cols:
        if k in raw_vals:
            print(f"  {k}: {raw_vals[k]:,.2f}")
else:
    print("  (Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø®Ø§Ù… ØºÙŠØ± Ù…ØªØ§Ø­Ø© ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§Ø› Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶ Ø£Ø¯Ù†Ø§Ù‡ Ù‚ÙŠÙÙ… Ù…ÙÙ‚Ø§Ø³Ø© z-score)")
    for k in raw_numeric_cols:
        if k in row_std.columns:
            print(f"  {k} (z): {float(row_std[k].iloc[0]):.3f}")

print(f"\nğŸ”® Model: {lbl}  (score={score:.4f})")
print(f"ğŸ“Œ Actual: {actual}  (y_test={y_true})")
print(f"ğŸ§ª Evaluation: {ok}")

# Binary flag >>> Ù‡Ù„ Ø¹Ù†Ø¯Ù‡ ØªØ¹Ø«Ù‘Ø± Ø³Ø§Ø¨Ù‚ Ù…Ø³Ø¬Ù‘Ù„ØŸ
# Band >>> Ø´Ø¯Ø© Ù…Ø®Ø§Ø·Ø±Ø©

# Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ø³Ø§Ù„Ø¨Ù‡ Ù‡ÙŠ Ø§Ù„Ù‚ÙŠÙ… Ø¨Ø¹Ø¯ Ù…Ø§ Ø§ØªØ¹Ù…Ù„Ù‡Ø§ >> Standardization

# Ù„ÙŠÙ‡ Ø¨Ù†Ø¹Ù…Ù„ Standardization Ù„Ù„Ù‚ÙŠÙ… â€œØ§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©â€ Ø²ÙŠ Ø§Ù„Ø¹Ù…Ø±/Ø§Ù„Ø¯Ø®Ù„/Ù…Ø¨Ù„Øº Ø§Ù„Ù‚Ø±Ø¶ØŸ
# Ù…Ø«Ù„Ù‹Ø§ Ø§Ù„Ø¯Ø®Ù„ Ù…Ù…ÙƒÙ† ÙŠØ¨Ù‚Ù‰ Ù…Ù† 2,000 Ù„Ù€ 200,000ØŒ Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„ÙØ§ÙŠØ¯Ø© Ù…Ù† 5 Ù„Ù€ 25 >>
# Ù„Ùˆ Ø³Ø¨ØªÙ‡Ù… Ø®Ø§Ù…ØŒ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø¨ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³Ø§ÙØ© Ø£Ùˆ Ø§Ù„Ø¥Ø³Ù‚Ø§Ø· Ù‡ØªØ®Ù„ÙŠ Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø¯Ù‰ (Ø§Ù„Ø¯Ø®Ù„) ØªÙ‡ÙŠÙ…Ù† Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Ø§Ø±

# ===== Mini GUI (self-healing) â€” trains if model not fitted =====
import numpy as np, pandas as pd, ipywidgets as W
from IPython.display import display, clear_output
from sklearn.utils.validation import check_is_fitted
import joblib

# -------- helpers: data & model ----------
def _find_first(names):
    for n in names:
        if n in globals():
            return globals()[n]
    return None

def _get_training_data():
    # Ù†ÙØ¶Ù‘Ù„ X_new / y_train Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯ÙŠÙ† (Ù†ÙØ³ Ø§Ù„Ù„ÙŠ Ø§ØªØ¯Ø±Ù‘Ø¨Øª Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ù†ÙˆØªØ¨ÙˆÙƒ)
    X = _find_first(['X_new','X'])
    y = _find_first(['y_train','y'])
    if X is None or y is None:
        raise AssertionError("Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¯Ø±ÙŠØ¨ (Ø¹Ø§ÙŠØ² X_new/y_train Ø£Ùˆ X/y). Ø´ØºÙ‘Ù„ Ø®Ù„Ø§ÙŠØ§ Ø§Ù„ØªØ­Ø¶ÙŠØ±/Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø£ÙˆÙ„.")
    # Ù„Ùˆ NumPy Ø­ÙˆÙ„Ù‡ DataFrame Ø¹Ø´Ø§Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
    if not isinstance(X, pd.DataFrame):
        cols = getattr(_find_first(['X_new_test','X_test']), 'columns', None)
        X = pd.DataFrame(X, columns=cols if cols is not None else [f'f{i}' for i in range(X.shape[1])])
    return X, np.array(y)

def _ensure_fitted_model():
    # Ø­Ø§ÙˆÙ„ ØªÙ„Ø§Ù‚ÙŠ Ù…ÙˆØ¯ÙŠÙ„ Ø¬Ø§Ù‡Ø²
    for name in ['xgb_final','ensemble','lgb','cat','xgb','final_model','best_xgb_model']:
        if name in globals():
            m = globals()[name]
            try:
                check_is_fitted(m)
                return m, name, False  # Ø¬Ø§Ù‡Ø²
            except Exception:
                candidate_name = name
                break
    else:
        candidate_name = None

    # Ù„Ùˆ ÙˆØµÙ„Ù†Ø§ Ù‡Ù†Ø§ ÙŠØ¨Ù‚Ù‰ Ù„Ø³Ù‡ Ù…ÙÙŠØ´ Ù…ÙˆØ¯ÙŠÙ„ fitted â€” Ø¬Ø±Ù‘Ø¨ Ù†Ø¯Ø±Ø¨ XGB Ø³Ø±ÙŠØ¹Ù‹Ø§
    X_tr, y_tr = _get_training_data()
    from xgboost import XGBClassifier

    # Ù„Ùˆ Ø¹Ù†Ø¯Ù†Ø§ best_xgb_params Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ØŒ ØºÙŠØ± ÙƒØ¯Ù‡ defaults Ø®ÙÙŠÙØ©
    params = {
        'random_state': 12, 'n_jobs': -1, 'tree_method': 'hist',
        'eval_metric': 'logloss', 'use_label_encoder': False
    }
    if 'best_xgb_params' in globals():
        params.update(globals()['best_xgb_params'])
    else:
        params.update(dict(
            n_estimators=400, max_depth=3, learning_rate=0.1,
            subsample=0.8, colsample_bytree=0.8
        ))
        # Ù„Ùˆ Ø¹Ù†Ø¯Ùƒ Ø¹Ø¯Ù… ØªÙˆØ§Ø²Ù† ÙƒØ¨ÙŠØ± ÙˆÙ…ØªÙˆÙØ± y:
        pos = int(np.sum(y_tr)); neg = len(y_tr) - pos
        if pos > 0:
            params['scale_pos_weight'] = neg / max(1, pos)

    m = XGBClassifier(**params)
    m.fit(X_tr, y_tr)
    joblib.dump(m, '/content/models/xgb_model.pkl')
    globals()['xgb_final'] = m
    return m, 'xgb_final', True  # Ø§ØªØ¯Ø±Ù‘Ø¨ Ø¯Ù„ÙˆÙ‚ØªÙŠ

def _feature_names_from_model_or_data(model):
    if hasattr(model, "feature_names_in_"):
        return list(model.feature_names_in_)
    if 'X_new' in globals() and isinstance(globals()['X_new'], pd.DataFrame):
        return list(globals()['X_new'].columns)
    if 'X_new_test' in globals() and isinstance(globals()['X_new_test'], pd.DataFrame):
        return list(globals()['X_new_test'].columns)
    raise AssertionError("Ù…Ø´ Ù„Ø§Ù‚ÙŠ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©. Ù„Ø§Ø²Ù… X_new/X_new_test Ø£Ùˆ model.feature_names_in_.")

def _find_scaler():
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ StandardScaler (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
    for obj in globals().values():
        if hasattr(obj, "mean_") and hasattr(obj, "scale_") and hasattr(obj, "feature_names_in_"):
            return obj
    return None

# -------- get model + feature names ----------
model, model_name, just_trained = _ensure_fitted_model()
feat_names = _feature_names_from_model_or_data(model)
scaler = _find_scaler()

# -------- categorical groups (one-hot) ----------
groups = {
    "Grade":          [c for c in ["A","B","C","D","E","F","G"] if c in feat_names],
    "Residence":      [c for c in ["MORTGAGE","RENT","OWN","OTHER"] if c in feat_names],
    "Purpose":        [c for c in ["DEBTCONSOLIDATION","EDUCATION","HOMEIMPROVEMENT","MEDICAL","PERSONAL","VENTURE"] if c in feat_names],
    "Age band":       [c for c in ["20-25","26-35","36-45","46-55","56-65"] if c in feat_names],
    "Size band":      [c for c in ["small","medium","large","very large"] if c in feat_names],
    "Binary flag":    [c for c in ["Y","N"] if c in feat_names],
    "Band":           [c for c in ["high","high-middle","middle","low-middle","low"] if c in feat_names],
}
groups = {k:v for k,v in groups.items() if v}  # Ø´ÙŠÙ„ Ø§Ù„ÙØ§Ø±ØºØ©

# -------- numeric inputs (RAW) ----------
numeric_raw = [
    ("person_age", "Ø§Ù„Ø¹ÙÙ…Ø± (Ø³Ù†Ø©)", 30.0),
    ("person_income", "Ø§Ù„Ø¯Ø®Ù„ Ø§Ù„Ø³Ù†ÙˆÙŠ", 30000.0),
    ("person_emp_length", "Ù…Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ (Ø³Ù†Ø©)", 3.0),
    ("loan_amnt", "Ù…Ø¨Ù„Øº Ø§Ù„Ù‚Ø±Ø¶", 10000.0),
    ("loan_int_rate", "Ø³Ø¹Ø± Ø§Ù„ÙØ§Ø¦Ø¯Ø© Ùª", 12.0),
]
raw_numeric_cols = [
    "person_age","person_income","person_emp_length","loan_amnt",
    "loan_int_rate","loan_percent_income","cb_person_cred_hist_length",
    "loan_to_income_ratio","loan_to_emp_length_ratio","int_rate_to_loan_amt_ratio"
]
maybe_index_col = 'index' if 'index' in feat_names else None

# -------- widgets ----------
cat_widgets = {g: W.Dropdown(options=opts, value=opts[0], description=g+":") for g,opts in groups.items()}
num_widgets = {k: W.FloatText(value=default, description=label+":") for k,label,default in numeric_raw}
btn = W.Button(description="Predict", button_style="primary")
out = W.Output()

# -------- build row ----------
def build_feature_row():
    v = pd.DataFrame([0]*len(feat_names), index=feat_names).T

    # one-hot
    for g, w in cat_widgets.items():
        cols = groups[g]
        v.loc[:, cols] = 0
        v.at[v.index[0], w.value] = 1

    # raw values
    vals = {k: w.value for k, w in num_widgets.items()}
    # Ù…Ø´ØªÙ‚Ø§Øª
    denom_income = max(1.0, vals.get("person_income", 0.0))
    loan_pct = vals.get("loan_amnt", 0.0) / denom_income
    cred_hist_len = 5.0  # ØªÙ‚Ø¯ÙŠØ± Ø¨Ø³ÙŠØ· Ù„Ùˆ Ù…ÙÙŠØ´ Ø¥Ø¯Ø®Ø§Ù„
    loan_to_emp = vals.get("loan_amnt", 0.0) / max(1.0, vals.get("person_emp_length", 0.0))
    int_to_loan = (vals.get("loan_int_rate", 0.0) / 100.0) / max(1.0, vals.get("loan_amnt", 0.0))

    raw_fill = {
        "person_age": vals.get("person_age", 0.0),
        "person_income": vals.get("person_income", 0.0),
        "person_emp_length": vals.get("person_emp_length", 0.0),
        "loan_amnt": vals.get("loan_amnt", 0.0),
        "loan_int_rate": vals.get("loan_int_rate", 0.0)/100.0,  # Ù„Ùˆ Ø¹Ù…ÙˆØ¯Ùƒ Ù…ÙØ®Ø²Ù‘Ù† ÙƒÙ†Ø³Ø¨Ø© Ø¹Ø´Ø±ÙŠØ©
        "loan_percent_income": loan_pct,
        "cb_person_cred_hist_length": cred_hist_len,
        "loan_to_income_ratio": loan_pct,
        "loan_to_emp_length_ratio": loan_to_emp,
        "int_rate_to_loan_amt_ratio": int_to_loan,
    }
    for k,val in raw_fill.items():
        if k in v.columns:
            v.at[v.index[0], k] = val

    if maybe_index_col:
        v.at[v.index[0], maybe_index_col] = 0

    # scaler Ø§Ø®ØªÙŠØ§Ø±ÙŠ
    if scaler is not None:
        common = [c for c in getattr(scaler, "feature_names_in_", []) if c in v.columns]
        for c in common:
            j = list(scaler.feature_names_in_).index(c)
            m = float(scaler.mean_[j]); s = float(scaler.scale_[j]) or 1.0
            v[c] = (v[c] - m) / s

    return v, raw_fill, scaler is not None

def predict_score(m, X):
    if hasattr(m, "predict_proba"):
        return float(m.predict_proba(X)[:, -1][0])
    if hasattr(m, "decision_function"):
        z = float(m.decision_function(X)[0])
        return 1/(1+np.exp(-z))
    return float(m.predict(X)[0])

# -------- click handler ----------
def on_click(_):
    with out:
        clear_output()
        try:
            X_row, raw_shown, used_scaler = build_feature_row()
            score = predict_score(model, X_row)
            pred = int(score >= 0.5)
            label = {0:"its safe", 1:"High Risk"}[pred]
            print(f"ğŸ§  Model: {model_name}   |   just_trained: {just_trained}   |   used_scaler: {used_scaler}")
            print(f"ğŸ”® Prediction: {label}  (score={score:.4f})")
            print("\nâ€” Entered RAW values â€”")
            for k, v in raw_shown.items():
                if k == "loan_int_rate":
                    print(f"  {k}: {v*100:.2f}%")
                else:
                    print(f"  {k}: {v:,.2f}")
        except Exception as e:
            print("âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£:", e)

btn.on_click(on_click)

# -------- render ----------
left  = W.VBox(list(cat_widgets.values()))
right = W.VBox(list(num_widgets.values()))
ui = W.HBox([left, W.Label('   '), right, W.Label('   '), btn])
display(ui, out)

